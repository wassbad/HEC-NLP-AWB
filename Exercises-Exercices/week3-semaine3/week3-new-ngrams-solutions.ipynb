{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd9d432-4e90-4f1e-9dfc-37f66a52e1a8",
   "metadata": {},
   "source": [
    "# Week 3: N-gram language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb198a98-901e-40b8-9887-65cffd5cf791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "eps = np.finfo(float).eps\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eedb2d-2fca-467c-bf43-1f22fd1919ee",
   "metadata": {},
   "source": [
    "The Brown Corpus comes preprocessed via word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "772be0d2-f300-49dc-bd50-f19d33aeb312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = brown.words()\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36d7b9-0223-42ed-943e-42210282dc00",
   "metadata": {},
   "source": [
    "For the purpose of experimentation, let's create a train/test split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b821aff8-e2d2-4862-925f-9fab3dfca229",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset[:1000000]\n",
    "test_data = dataset[1000000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f798d5-78d0-4a6a-9b4a-1b7797260b90",
   "metadata": {},
   "source": [
    "### Train uni-gram language model\n",
    "\n",
    "Let's now start by implementing a bag-of-words, or our unigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa1ea9c3-e72c-42b7-9f4d-f69039eb3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_vocabulary(dataset):\n",
    "    types = list(set(dataset))\n",
    "    return types\n",
    "\n",
    "def unigram_lm(sequence_tokens, vocabulary):\n",
    "    BoW = {t: 0 for t in vocabulary}\n",
    "    counts = dict(Counter(sequence_tokens))\n",
    "    total = sum(counts.values())\n",
    "    for token in BoW:\n",
    "        if token in counts:\n",
    "            BoW[token] = counts[token]/total + eps\n",
    "        else:\n",
    "            BoW[token] = eps\n",
    "    return BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48874e5b-420f-4ed5-8509-e9528fe61104",
   "metadata": {},
   "source": [
    "Let's fit our unigram model to our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b177bb-d70b-4d74-8fbf-91834503ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_unigrams = unigram_lm(train_data, get_unigram_vocabulary(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25903556-5d70-4dd1-8255-46aae4b7b60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.400000000022204e-05"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_unigrams[\"horse\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce1d27-e79f-43e5-acaa-b7cb589ce184",
   "metadata": {},
   "source": [
    "### Train an bi-gram language model\n",
    "\n",
    "Now let's write a function that returns a bigram model. The first step is a function that returns the set of possible bigrams in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2588ce01-2810-456b-b2bb-9ea667bfbf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_vocabulary(dataset):\n",
    "    bigram_types = []\n",
    "    pad_token = \"[PAD]\"\n",
    "    ## TO DO\n",
    "    prev_token = pad_token\n",
    "    for token in dataset:\n",
    "        bigram_types.append((prev_token, token))\n",
    "        prev_token = token\n",
    "    bigram_types = list(set(bigram_types))\n",
    "    ##\n",
    "    return bigram_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d971f736-c811-44cd-af7a-8a5371460ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455268"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = get_bigram_vocabulary(dataset)\n",
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e19c36-816e-4979-8e93-6afcd325eca3",
   "metadata": {},
   "source": [
    "Now that we have a way to get the set of bigram types lets write the bigram model (don't forget to implement smoothing by adding eps to all probability values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76e98fe-5045-4a60-8578-47a7029e6418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_lm(train_data, dataset):\n",
    "    bigrams = get_bigram_vocabulary(dataset)\n",
    "    unigrams = get_unigram_vocabulary(dataset)+[\"[PAD]\"]\n",
    "    bigram_counts = {t: 0 for t in bigrams}\n",
    "    unigram_counts = {t: 0 for t in unigrams}\n",
    "    unigram_counts[\"[PAD]\"] = 1\n",
    "    bigram_probs = dict()\n",
    "    ## TO DO\n",
    "    sequence_tokens = [\"[PAD]\"] + train_data\n",
    "    for i in range(len(sequence_tokens)-1):\n",
    "        bigram = (sequence_tokens[i], sequence_tokens[i+1])\n",
    "        bigram_counts[bigram] += 1\n",
    "        unigram_counts[sequence_tokens[i]] +=1\n",
    "    for bigram in bigram_counts:\n",
    "        if bigram_counts[bigram] > 0:\n",
    "            bigram_probs[bigram] = bigram_counts[bigram]/unigram_counts[bigram[0]] + eps\n",
    "        else:\n",
    "            bigram_probs[bigram] = eps\n",
    "    ##\n",
    "    return bigram_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc91426-9282-4492-a847-662c21be580a",
   "metadata": {},
   "source": [
    "Let's fit a bigram model to the brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "046bcbfe-5431-4160-8b09-08baec9345a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_bigrams = bigram_lm(train_data, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b1983-b058-4e33-b651-c45a77b5ff7f",
   "metadata": {},
   "source": [
    "### Train a tri-gram language model\n",
    "\n",
    "Lets now repeat these steps but for a trigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe3324b8-0012-4538-8e47-9d07c6251ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigram_vocabulary(dataset):\n",
    "    trigram_types = []\n",
    "    pad_token = \"[PAD]\"\n",
    "    ## TO DO\n",
    "    prev_token_1 = pad_token\n",
    "    prev_token_2 = pad_token\n",
    "    for token in dataset:\n",
    "        trigram_types.append((prev_token_2, prev_token_1, token))\n",
    "        prev_token_2 = prev_token_1\n",
    "        prev_token_1 = token\n",
    "    trigram_types = list(set(trigram_types))\n",
    "    ##\n",
    "    return trigram_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1f83483-8994-47ba-b881-744a17f74727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_lm(train_data, dataset):\n",
    "    trigrams = get_trigram_vocabulary(dataset)\n",
    "    bigrams = get_bigram_vocabulary(dataset)+[(\"[PAD]\",\"[PAD]\")]\n",
    "    trigram_counts = {t: 0 for t in trigrams}\n",
    "    bigram_counts = {t: 0 for t in bigrams}\n",
    "    trigram_probs = dict()\n",
    "    ## TO DO\n",
    "    sequence_tokens = [\"[PAD]\", \"[PAD]\"] + train_data\n",
    "    for i in range(len(sequence_tokens)-2):\n",
    "        trigram = (sequence_tokens[i], sequence_tokens[i+1], sequence_tokens[i+2])\n",
    "        bigram = (sequence_tokens[i], sequence_tokens[i+1])\n",
    "        trigram_counts[trigram] += 1\n",
    "        bigram_counts[bigram] += 1\n",
    "    for trigram in trigram_counts:\n",
    "        if trigram_counts[trigram] > 0:\n",
    "            trigram_probs[trigram] = trigram_counts[trigram]/bigram_counts[(trigram[0],trigram[1])] + eps\n",
    "        else:\n",
    "            trigram_probs[trigram] = eps\n",
    "    ##\n",
    "    return trigram_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702c339-5b9e-4d0d-85de-61890c5bb5ab",
   "metadata": {},
   "source": [
    "Let's fit a trigram model to the brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3f72911-c1e9-4397-8325-36e2e8dfe92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_trigrams = trigram_lm(train_data, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33937e13-1379-46a6-bffe-8e07ad12f939",
   "metadata": {},
   "source": [
    "### Compare the perplexity of the test data of each model\n",
    "\n",
    "Which of these models performs best at representing the test data distribution? Write a function that takes a fitted ngram model and a test dataset and returns the perplexity of that dataset. \n",
    "\n",
    "Since the probability of the test data is the product of the probabilities of the ngrams which compose it, it is a very small number and we risk running into a floating-point error when trying to compute it. Thus, we should calculate perplexity in log base 2 space. Here is the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2c6bd-393f-45e4-89db-192961152c9a",
   "metadata": {},
   "source": [
    "$PP(W) = 2^{-\\frac{1}{n}\\log P(W)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67663da8-0b9e-43ad-b7ee-f6c8767fd8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(ngram_lm, test_data):\n",
    "    if type(list(ngram_lm.keys())[0]) is tuple :\n",
    "        ngram_size = len(list(ngram_lm.keys())[0])\n",
    "    else:\n",
    "        ngram_size = 1\n",
    "    perplexity = 0.0\n",
    "    n = len(test_data)+(ngram_size-1)\n",
    "    ## TO DO\n",
    "    cum_prob = 0.0\n",
    "    for i in range(len(test_data)-(ngram_size+1)):\n",
    "        if ngram_size > 1: \n",
    "            ngram = tuple(test_data[i:(i+ngram_size)])\n",
    "        else:\n",
    "            ngram = test_data[i]\n",
    "        prob = ngram_lm[ngram]\n",
    "        cum_prob = cum_prob + math.log(prob)\n",
    "    perplexity = 2**((-1/n)*cum_prob)\n",
    "    ##\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f34a8dac-6458-44e3-88e8-c0126e304d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4270349833895764"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_perplexity(brown_trigrams, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0eea59f0-ff16-4d86-b287-91805974b6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>train_perplexity</th>\n",
       "      <th>test_perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unigram</td>\n",
       "      <td>144.946420</td>\n",
       "      <td>2.551061e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bigram</td>\n",
       "      <td>20.556295</td>\n",
       "      <td>2.166769e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trigram</td>\n",
       "      <td>3.427035</td>\n",
       "      <td>1.190379e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    models  train_perplexity  test_perplexity\n",
       "0  unigram        144.946420     2.551061e+02\n",
       "1   bigram         20.556295     2.166769e+04\n",
       "2  trigram          3.427035     1.190379e+08"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_perplexity_scores(models, dataset):\n",
    "    results = [get_perplexity(lm, dataset) for lm in models]\n",
    "    return results\n",
    "\n",
    "models = [brown_unigrams, brown_bigrams, brown_trigrams]\n",
    "train_perplexity = compare_perplexity_scores(models, train_data)\n",
    "test_perplexity = compare_perplexity_scores(models, test_data)\n",
    "\n",
    "results = {'models':['unigram','bigram','trigram'],\n",
    " 'train_perplexity':train_perplexity,\n",
    " 'test_perplexity':test_perplexity}\n",
    "\n",
    "df_results = pd.DataFrame(data=results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa3b8b-df60-4a0d-8017-7e54a6bcd64b",
   "metadata": {},
   "source": [
    "What do you notice about these results? Why might that be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
